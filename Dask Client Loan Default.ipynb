{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#create DASK scheduler and worker remote containers\n",
    "#this will take at least one minute as there are delays added on purpose to allow containers to spawn\n",
    "#on successful run you should see scheduler URL printed\n",
    "\n",
    "#!python3 daskmaster.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined as global\n",
    "daskschurl = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsw\n",
    "import os\n",
    "import time\n",
    "\n",
    "def dask_distributed_launch(nworkers=1):\n",
    "    # modify global copy\n",
    "    global daskschurl\n",
    "    \n",
    "    #check if already running\n",
    "    if daskschurl!=\"\":\n",
    "        #print(cdsw.list_workers())\n",
    "        print(\" Dask Scheduler Already Launched \" + daskschurl)\n",
    "        return(daskschurl)\n",
    "    \n",
    "    # Launch CDSW workers. These are engines that will run in \n",
    "    # the same project, execute a given code or script, and exit.\n",
    "    # Scheduler engine will keep running in background until session is closed\n",
    "    dask_scheduler = cdsw.launch_workers(n=1, cpu=2, memory=4, \n",
    "                                  kernel=\"python3\",script=\"daskschedular.py\")\n",
    "\n",
    "\n",
    "    # IP of launched container comes up unknown for a while\n",
    "    # Wait for a while so IP is available in data structure\n",
    "    time.sleep(30)\n",
    "\n",
    "    # Get schedular IP\n",
    "    schedulerid = dask_scheduler[0][\"id\"]\n",
    "    listtemp = cdsw.list_workers()\n",
    "\n",
    "    for x in listtemp:\n",
    "      if x[\"id\"] == schedulerid:\n",
    "        schedulerip = x[\"ip_address\"]\n",
    "\n",
    "\n",
    "    print(\" Scheduler IP: \" + schedulerip)\n",
    "\n",
    "    #Scheduler protocol and port - defaults from Dask\n",
    "    schproto = \"tcp://\"\n",
    "    schport = \":8786\"\n",
    "\n",
    "    schloc = schproto + schedulerip + schport\n",
    "    print(\" Scheduler URL: \" + schloc)\n",
    "\n",
    "    dask_client = []\n",
    "    # Launch at least one Dask Worker\n",
    "    for c in range(nworkers):\n",
    "        dask_client = dask_client + cdsw.launch_workers(n=1, cpu=2, memory=4, \n",
    "                                  kernel=\"python3\",script=\"daskworker.py\",\n",
    "                                      env={\"DASKSCHURL\": schloc})\n",
    "\n",
    "        # wait for a while until the container is launched successfully\n",
    "        time.sleep(10)\n",
    "    \n",
    "    #set scheduler URL as environment variable\n",
    "    #os.putenv(\"DASKSCHURL\", schloc)\n",
    "    daskschurl = schloc\n",
    "\n",
    "    #return scheduler URL\n",
    "    return(schloc)\n",
    "\n",
    "\n",
    "def dask_stop_workers():\n",
    "    global daskschurl\n",
    "    cdsw.stop_workers()\n",
    "    daskschurl = \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dask_test():\n",
    "    from dask.distributed import Client\n",
    "    client = Client(daskschurl)\n",
    "    import dask.array as da\n",
    "    x = da.random.random((40000,40000),chunks=(1000,1000))\n",
    "    y = da.exp(x).sum()\n",
    "    print(\" Result of DASK distributed array test: \" + str(y.compute()) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop any previous dask distributed containers\n",
    "dask_stop_workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Scheduler IP: 10.10.9.12\n",
      " Scheduler URL: tcp://10.10.9.12:8786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tcp://10.10.9.12:8786'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#launch scheduler and worker container(s) - you can specify number of workers as argument\n",
    "#to relaunch - first call dask_stop_workers() and then call this again\n",
    "dask_distributed_launch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcp://10.10.9.12:8786\n"
     ]
    }
   ],
   "source": [
    "#check if global variable has the right URL\n",
    "#we will use this to register a client\n",
    "print(daskschurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === List of launched running pods === \n",
      " ===      (scheduler + workers)    === \n",
      "                 Id   IP Address  CPUs  Memory   Status\n",
      "0  ffhaevzrunb0p282  10.10.17.21     2       4  running\n",
      "1  xzijj9820q0n5gl6  10.10.17.99     2       4  running\n",
      "2  lwqhww0d7xj9q4lk   10.10.9.12     2       4  running\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cdsw\n",
    "#any previously stopped pods may show up with status failed\n",
    "#we are not showing those\n",
    "workers_list = cdsw.list_workers()\n",
    "print(\" === List of launched running pods === \")\n",
    "print(\" ===      (scheduler + workers)    === \")\n",
    "\n",
    "#collect relevant fields\n",
    "workersl = []\n",
    "for l in workers_list:\n",
    "    #print(l)\n",
    "    #print(\" id: \" + l[\"id\"] + \" IP Addr: \" + l[\"ip_address\"] + \" CPUs: \" + str(l[\"cpu\"]) + \" Memory: \" + str(l[\"memory\"]) \\\n",
    "    #     + \" Status: \" + l[\"status\"])\n",
    "    workersl = workersl + [[l[\"id\"],l[\"ip_address\"],l[\"cpu\"],l[\"memory\"],l[\"status\"]]]\n",
    "\n",
    "workersactive = pd.DataFrame(workersl,columns=[\"Id\",\"IP Address\",\"CPUs\",\"Memory\",\"Status\"])\n",
    "print(workersactive[workersactive.Status==\"running\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Result of DASK distributed array test: 2749240235.32\n",
      "\n",
      "CPU times: user 5.84 s, sys: 64.1 ms, total: 5.91 s\n",
      "Wall time: 9.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#check distributed dask is working\n",
    "dask_test()\n",
    "\n",
    "#Register a DASK client and run a test\n",
    "#from dask.distributed import Client\n",
    "#client = Client(daskschurl)\n",
    "#import dask.array as da\n",
    "#x = da.random.random((40000,40000),chunks=(1000,1000))\n",
    "#y = da.exp(x).sum()\n",
    "#print(\"DASK test result: \") \n",
    "#print(y.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 545 ms, sys: 91.5 ms, total: 636 ms\n",
      "Wall time: 7.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Read file from S3\n",
    "#configure access via aws configure in terminal\n",
    "#need pip3 install awscli for it to work\n",
    "\n",
    "#Dask dataframe has poor ability to infer column types\n",
    "#Need to be fixed manually\n",
    "#options can be modified in ~/.config/dask/distributed.yaml \n",
    "\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import s3fs\n",
    "df = dd.read_csv(\"s3://harshalpatil-s3/loans_accepted_2007_to_2018Q4.csv\", \\\n",
    "                blocksize=\"25MB\",sample=1000000,dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check number of partitions \n",
    "\n",
    "#df.known_divisions\n",
    "#df.set_index(\"grade\")\n",
    "#df = df.repartition(npartitions=10000)\n",
    "df.npartitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataframe: \n",
      "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
      "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
      "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
      "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
      "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
      "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
      "\n",
      "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
      "0        3.06                  0.28             2.29             5.64  1.04   \n",
      "1        2.76                  0.26             1.28             4.38  1.05   \n",
      "2        3.24                  0.30             2.81             5.68  1.03   \n",
      "3        3.49                  0.24             2.18             7.80  0.86   \n",
      "4        2.69                  0.39             1.82             4.32  1.04   \n",
      "\n",
      "   od280/od315_of_diluted_wines  proline  \n",
      "0                          3.92   1065.0  \n",
      "1                          3.40   1050.0  \n",
      "2                          3.17   1185.0  \n",
      "3                          3.45   1480.0  \n",
      "4                          2.93    735.0  \n",
      "\n",
      " Target: \n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      " === Xgboost Classifier Feature Importance: === \n",
      "    classifier_feature_importance                      variable\n",
      "11                       0.308533  od280/od315_of_diluted_wines\n",
      "12                       0.178464                       proline\n",
      "9                        0.162332               color_intensity\n",
      "6                        0.150665                    flavanoids\n",
      "1                        0.074343                    malic_acid\n",
      "4                        0.036550                     magnesium\n",
      "0                        0.027853                       alcohol\n",
      "8                        0.016394               proanthocyanins\n",
      "10                       0.014062                           hue\n",
      "5                        0.012643                 total_phenols\n",
      "2                        0.009647                           ash\n",
      "3                        0.004542             alcalinity_of_ash\n",
      "7                        0.003973          nonflavanoid_phenols\n",
      "\n",
      " Sample initial five predictions: \n",
      "[0 0 0 0 1]\n",
      "\n",
      " Check classes other than zero predicted: \n",
      "[1 1 1 1 1 1 1 1 2 2 2 2 1 2 2]\n",
      "\n",
      "\n",
      " Model Accuracy: \n",
      "0.947368421053\n",
      "\n",
      " === End Dask Xgboost === \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Use the wine dataset\n",
    "from sklearn.datasets import load_wine\n",
    "data = load_wine()\n",
    "\n",
    "from dask import dataframe as dd\n",
    "#since the data is numpy series we will not use df = dd.from_pandas(data[‘data’])\n",
    "#dask has various ways to convert numpy and pandas to dask dataframes      \n",
    "df = dd.from_array(data['data'])\n",
    "df.columns = data['feature_names']\n",
    "\n",
    "#print a few lines\n",
    "print(\"\\n Dataframe: \")\n",
    "print(df.head())\n",
    "\n",
    "#Get target variable\n",
    "dt = dd.from_array(data['target'])\n",
    "dt.columns = [\"target\"]\n",
    "      \n",
    "#print target classes example\n",
    "print(\"\\n Target: \")\n",
    "print(dt.head())      \n",
    "\n",
    "# train and test split\n",
    "from dask_ml.model_selection import train_test_split\n",
    "train, test, train_labels, test_labels = train_test_split(df,dt,random_state=123)      \n",
    "      \n",
    "#xgboost\n",
    "from dask_ml.xgboost import XGBClassifier\n",
    "est = XGBClassifier()      \n",
    "      \n",
    "#fit model      \n",
    "model = est.fit(train, train_labels)\n",
    "\n",
    "#which features contribute most\n",
    "import pandas as pd\n",
    "featureimp = pd.DataFrame(model.feature_importances_)\n",
    "featureimp.columns = ['classifier_feature_importance']\n",
    "featureimp[\"variable\"] = data['feature_names']\n",
    "print(\"\\n\\n === Xgboost Classifier Feature Importance: === \")\n",
    "print(featureimp.sort_values(by=\"classifier_feature_importance\", ascending=False))\n",
    "#featureimp.to_csv()\n",
    "\n",
    "\n",
    "#predictions\n",
    "ypred = model.predict(test)\n",
    "\n",
    "#sample some predictions\n",
    "print(\"\\n Sample initial five predictions: \")      \n",
    "print(ypred[[0,1,2,3,4]].compute())\n",
    "\n",
    "#ensure model is predicting all classes - not just 0\n",
    "print(\"\\n Check classes other than zero predicted: \")\n",
    "print(ypred[ypred>0].compute())\n",
    "      \n",
    "#check accuracy on test set      \n",
    "from dask_ml import metrics\n",
    "print(\"\\n\\n Model Accuracy: \")      \n",
    "print(metrics.accuracy_score(test_labels,model.predict(test)))\n",
    "      \n",
    "print(\"\\n === End Dask Xgboost === \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
